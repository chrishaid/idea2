# A redacted version of airflow.cfg

[core]
airflow_home = /etc/airflow
dags_folder = /etc/airflow/dags
base_log_folder = /var/log/airflow
s3_log_folder = None
#remote_base_log_folder = gs://some-bucket
#remote_log_conn_id = gcp_di
executor = LocalExecutor
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres/airflow
sql_alchemy_pool_size = 5
sql_alchemy_pool_recycle = 3600
parallelism = 32
dag_concurrency = 16
max_active_runs_per_dag = 16
load_examples = False
plugins_folder = /etc/airflow/plugins
#fernet_key = kYaLE7G8dtKsVMfoIuXGl2kLvvDOzkEg0lI9ssKy7N4=
donot_pickle = False
dags_are_paused_at_creation = True



[webserver]
base_url =  http://localhost:8080
web_server_host = 0.0.0.0
web_server_port = 8080
#secret_key = test
workers = 4
worker_class = sync
expose_config = false
#authenticate = False
#auth_backend = airflow.contrib.auth.backends.ldap_auth
filter_by_owner = False

[scheduler]
job_heartbeat_sec = 10
scheduler_heartbeat_sec = 10

# after how much time a new DAGs should be picked up from the filesystem
min_file_process_interval = 0

dag_dir_list_interval = 300

# How often should stats be printed to the logs
print_stats_interval = 30

child_process_log_directory = /var/log/airflow/

# don't backfill
catchup_by_default = False
